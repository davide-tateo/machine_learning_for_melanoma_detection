{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eecbc8db",
   "metadata": {
    "papermill": {
     "duration": 0.004634,
     "end_time": "2024-05-09T20:22:05.309483",
     "exception": false,
     "start_time": "2024-05-09T20:22:05.304849",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Final Exam Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e23a61",
   "metadata": {
    "papermill": {
     "duration": 0.003633,
     "end_time": "2024-05-09T20:22:05.317362",
     "exception": false,
     "start_time": "2024-05-09T20:22:05.313729",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Davide Tateo; 167275\n",
    "\n",
    "Francesca Salute; 167284\n",
    "\n",
    "Nicole Favero; 167340 \n",
    "\n",
    "Tomás Gonçalves; 167288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01e33743",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T20:22:05.327244Z",
     "iopub.status.busy": "2024-05-09T20:22:05.326592Z",
     "iopub.status.idle": "2024-05-09T20:22:30.683927Z",
     "shell.execute_reply": "2024-05-09T20:22:30.682858Z"
    },
    "papermill": {
     "duration": 25.36528,
     "end_time": "2024-05-09T20:22:30.686398",
     "exception": false,
     "start_time": "2024-05-09T20:22:05.321118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ML_FinalExam'...\r\n",
      "remote: Enumerating objects: 22331, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (8143/8143), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (8128/8128), done.\u001b[K\r\n",
      "remote: Total 22331 (delta 22), reused 8132 (delta 14), pack-reused 14188\u001b[K\r\n",
      "Receiving objects: 100% (22331/22331), 401.90 MiB | 35.18 MiB/s, done.\r\n",
      "Resolving deltas: 100% (33/33), done.\r\n",
      "Updating files: 100% (22257/22257), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/frasalute/ML_FinalExam.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5980d48b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T20:22:30.726224Z",
     "iopub.status.busy": "2024-05-09T20:22:30.725459Z",
     "iopub.status.idle": "2024-05-09T20:22:30.730890Z",
     "shell.execute_reply": "2024-05-09T20:22:30.729991Z"
    },
    "papermill": {
     "duration": 0.02752,
     "end_time": "2024-05-09T20:22:30.733003",
     "exception": false,
     "start_time": "2024-05-09T20:22:30.705483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /kaggle/working\n",
      "Contents of the current directory: ['__notebook__.ipynb', 'ML_FinalExam']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Contents of the current directory: {os.listdir('.')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7684923f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T20:22:30.780502Z",
     "iopub.status.busy": "2024-05-09T20:22:30.780150Z",
     "iopub.status.idle": "2024-05-09T20:22:30.785545Z",
     "shell.execute_reply": "2024-05-09T20:22:30.784568Z"
    },
    "papermill": {
     "duration": 0.037028,
     "end_time": "2024-05-09T20:22:30.788959",
     "exception": false,
     "start_time": "2024-05-09T20:22:30.751931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base path is: /kaggle/working/ML_FinalExam/images\n"
     ]
    }
   ],
   "source": [
    "base_path = '/kaggle/working/ML_FinalExam/images'\n",
    "print(\"Base path is:\", base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82ebc5a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T20:22:30.838126Z",
     "iopub.status.busy": "2024-05-09T20:22:30.837731Z",
     "iopub.status.idle": "2024-05-09T20:22:43.973754Z",
     "shell.execute_reply": "2024-05-09T20:22:43.972634Z"
    },
    "papermill": {
     "duration": 13.163557,
     "end_time": "2024-05-09T20:22:43.976467",
     "exception": false,
     "start_time": "2024-05-09T20:22:30.812910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-09 20:22:32.721789: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-09 20:22:32.721886: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-09 20:22:32.858220: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d369b104",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T20:22:44.013462Z",
     "iopub.status.busy": "2024-05-09T20:22:44.012818Z",
     "iopub.status.idle": "2024-05-09T20:22:44.022184Z",
     "shell.execute_reply": "2024-05-09T20:22:44.021344Z"
    },
    "papermill": {
     "duration": 0.030045,
     "end_time": "2024-05-09T20:22:44.024258",
     "exception": false,
     "start_time": "2024-05-09T20:22:43.994213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#importing required API and modules\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.utils import load_img\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72148f3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T20:22:44.060871Z",
     "iopub.status.busy": "2024-05-09T20:22:44.060359Z",
     "iopub.status.idle": "2024-05-09T20:23:00.991550Z",
     "shell.execute_reply": "2024-05-09T20:23:00.990675Z"
    },
    "papermill": {
     "duration": 16.952118,
     "end_time": "2024-05-09T20:23:00.994037",
     "exception": false,
     "start_time": "2024-05-09T20:22:44.041919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_images_to_df(base_path):\n",
    "    data = {'photo_id': [], 'image': [], 'image_array' :[], 'malignant': []}\n",
    "    categories = {'Benign': 0, 'Malignant': 1}\n",
    "    \n",
    "    for subset in ['train', 'test']:\n",
    "        for category in ['Benign', 'Malignant']:\n",
    "            folder_path = os.path.join(base_path, subset, category)\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith('.jpg'):\n",
    "                    file_path = os.path.join(folder_path, filename)\n",
    "                    # Upload Images\n",
    "                    image = load_img(file_path)\n",
    "                    image_array = img_to_array(image)\n",
    "                    # Add the data\n",
    "                    data['photo_id'].append(filename)\n",
    "                    data['image'].append(image)\n",
    "                    data['image_array'].append(image_array)\n",
    "                    data['malignant'].append(categories[category])\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_images_to_df(base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "645c51e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T20:23:01.033306Z",
     "iopub.status.busy": "2024-05-09T20:23:01.032918Z",
     "iopub.status.idle": "2024-05-09T20:23:09.111831Z",
     "shell.execute_reply": "2024-05-09T20:23:09.110887Z"
    },
    "papermill": {
     "duration": 8.100521,
     "end_time": "2024-05-09T20:23:09.114178",
     "exception": false,
     "start_time": "2024-05-09T20:23:01.013657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   photo_id                                              image  \\\n",
      "0  5366.jpg  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n",
      "1  4347.jpg  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n",
      "2    90.jpg  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n",
      "3  5524.jpg  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n",
      "4  5006.jpg  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n",
      "\n",
      "                                         image_array  malignant  \n",
      "0  [[[147.0, 128.0, 132.0], [150.0, 131.0, 135.0]...          0  \n",
      "1  [[[189.0, 120.0, 139.0], [188.0, 119.0, 138.0]...          0  \n",
      "2  [[[242.0, 167.0, 188.0], [241.0, 166.0, 187.0]...          0  \n",
      "3  [[[196.0, 154.0, 166.0], [195.0, 153.0, 165.0]...          0  \n",
      "4  [[[181.0, 109.0, 133.0], [179.0, 107.0, 131.0]...          0  \n",
      "Number of rows in the DataFrame: 13879\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "row_count = len(df)\n",
    "print(\"Number of rows in the DataFrame:\", row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0345a2",
   "metadata": {
    "papermill": {
     "duration": 0.016977,
     "end_time": "2024-05-09T20:23:09.148992",
     "exception": false,
     "start_time": "2024-05-09T20:23:09.132015",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Code to make augmentation of pics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "054e9a51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T20:23:09.190964Z",
     "iopub.status.busy": "2024-05-09T20:23:09.190567Z",
     "iopub.status.idle": "2024-05-09T20:23:09.202049Z",
     "shell.execute_reply": "2024-05-09T20:23:09.201118Z"
    },
    "papermill": {
     "duration": 0.035787,
     "end_time": "2024-05-09T20:23:09.204724",
     "exception": false,
     "start_time": "2024-05-09T20:23:09.168937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'original_dataset_dir = \\'./images/undetec_to_augm\\'\\ntrain_augm_undetected_dir = \\'./images/train/Undetected\\'\\ntest_augm_undetected_dir =\\'./images/test/Undetected\\'\\n\\n# Create a data generator for augmentation\\ndatagen = ImageDataGenerator(\\n    rotation_range=30,\\n    width_shift_range=0.2,\\n    height_shift_range=0.2,\\n    shear_range=0.2,\\n    zoom_range=0.2,\\n    horizontal_flip=True,\\n    brightness_range = [0.8, 1.3],\\n    fill_mode=\\'nearest\\'\\n)\\n\\n# List the files in the original dataset directory\\nfile_list = os.listdir(original_dataset_dir)\\n\\n# Ensure the target directories exist\\nos.makedirs(train_augm_undetected_dir, exist_ok=True)\\nos.makedirs(test_augm_undetected_dir, exist_ok=True)\\n\\n# Desired number of images after augmentation\\ntarget_count_1 = 6000\\ntarget_count_2 = 1000\\n\"\"\"= target_count_2 = target_count_3 = target_count_4 = target_count_5 = target_count_6 = target_count_7\"\"\"\\n\\n# Number of images in the original dataset\\noriginal_count = len(file_list)\\n\\n\\n# Number of times to repeat each image to reach the target count\\nrepeats_1 = min(target_count_1 // original_count + 1, len(file_list))\\nrepeats_2 = min(target_count_2 // original_count + 1, len(file_list))\\n\"\"\"repeats_3 = min(target_count_3 // original_count + 1, len(file_list))\\nrepeats_4 = min(target_count_4 // original_count + 1, len(file_list))\\nrepeats_5 = min(target_count_5 // original_count + 1, len(file_list))\\nrepeats_6 = min(target_count_6 // original_count + 1, len(file_list))\\nrepeats_7 = min(target_count_7 // original_count + 1, len(file_list))\"\"\"\\n\\n# Augment and save the images for the 6000 train images\\nfor file in file_list[:original_count]:\\n    img_path = os.path.join(original_dataset_dir, file)\\n    img = load_img(img_path)\\n    img = img_to_array(img)\\n    img = img.reshape((1,) + img.shape)\\n\\n    i = 0\\n    for batch in datagen.flow(img, batch_size=1, save_to_dir = train_augm_undetected_dir, save_prefix=\\'augm\\', save_format=\\'jpeg\\'): \\n        #if in the previous line i save them as jpg, it anyway augments only the jpeg ones\\n        i += 1\\n        if i >= repeats_1:\\n            break  # break the loop after reaching the desired number of augmented images\\n\\n\\n\\n# Augment and save the images for the 1000 test images\\nfor file in file_list[:original_count]:\\n    img_path = os.path.join(original_dataset_dir, file)\\n    img = load_img(img_path)\\n    img = img_to_array(img)\\n    img = img.reshape((1,) + img.shape)\\n\\n    i = 0\\n    for batch in datagen.flow(img, batch_size=1, save_to_dir = test_augm_undetected_dir, save_prefix=\\'aug\\', save_format=\\'jpeg\\'):\\n        i += 1\\n        if i >= repeats_2:\\n            break  # break the loop after reaching the desired number of augmented images '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''original_dataset_dir = './images/undetec_to_augm'\n",
    "train_augm_undetected_dir = './images/train/Undetected'\n",
    "test_augm_undetected_dir ='./images/test/Undetected'\n",
    "\n",
    "# Create a data generator for augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range = [0.8, 1.3],\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# List the files in the original dataset directory\n",
    "file_list = os.listdir(original_dataset_dir)\n",
    "\n",
    "# Ensure the target directories exist\n",
    "os.makedirs(train_augm_undetected_dir, exist_ok=True)\n",
    "os.makedirs(test_augm_undetected_dir, exist_ok=True)\n",
    "\n",
    "# Desired number of images after augmentation\n",
    "target_count_1 = 6000\n",
    "target_count_2 = 1000\n",
    "\"\"\"= target_count_2 = target_count_3 = target_count_4 = target_count_5 = target_count_6 = target_count_7\"\"\"\n",
    "\n",
    "# Number of images in the original dataset\n",
    "original_count = len(file_list)\n",
    "\n",
    "\n",
    "# Number of times to repeat each image to reach the target count\n",
    "repeats_1 = min(target_count_1 // original_count + 1, len(file_list))\n",
    "repeats_2 = min(target_count_2 // original_count + 1, len(file_list))\n",
    "\"\"\"repeats_3 = min(target_count_3 // original_count + 1, len(file_list))\n",
    "repeats_4 = min(target_count_4 // original_count + 1, len(file_list))\n",
    "repeats_5 = min(target_count_5 // original_count + 1, len(file_list))\n",
    "repeats_6 = min(target_count_6 // original_count + 1, len(file_list))\n",
    "repeats_7 = min(target_count_7 // original_count + 1, len(file_list))\"\"\"\n",
    "\n",
    "# Augment and save the images for the 6000 train images\n",
    "for file in file_list[:original_count]:\n",
    "    img_path = os.path.join(original_dataset_dir, file)\n",
    "    img = load_img(img_path)\n",
    "    img = img_to_array(img)\n",
    "    img = img.reshape((1,) + img.shape)\n",
    "\n",
    "    i = 0\n",
    "    for batch in datagen.flow(img, batch_size=1, save_to_dir = train_augm_undetected_dir, save_prefix='augm', save_format='jpeg'): \n",
    "        #if in the previous line i save them as jpg, it anyway augments only the jpeg ones\n",
    "        i += 1\n",
    "        if i >= repeats_1:\n",
    "            break  # break the loop after reaching the desired number of augmented images\n",
    "\n",
    "\n",
    "\n",
    "# Augment and save the images for the 1000 test images\n",
    "for file in file_list[:original_count]:\n",
    "    img_path = os.path.join(original_dataset_dir, file)\n",
    "    img = load_img(img_path)\n",
    "    img = img_to_array(img)\n",
    "    img = img.reshape((1,) + img.shape)\n",
    "\n",
    "    i = 0\n",
    "    for batch in datagen.flow(img, batch_size=1, save_to_dir = test_augm_undetected_dir, save_prefix='aug', save_format='jpeg'):\n",
    "        i += 1\n",
    "        if i >= repeats_2:\n",
    "            break  # break the loop after reaching the desired number of augmented images '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d320fdb0",
   "metadata": {
    "papermill": {
     "duration": 0.020456,
     "end_time": "2024-05-09T20:23:09.245954",
     "exception": false,
     "start_time": "2024-05-09T20:23:09.225498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c992e",
   "metadata": {
    "papermill": {
     "duration": 0.020219,
     "end_time": "2024-05-09T20:23:09.286611",
     "exception": false,
     "start_time": "2024-05-09T20:23:09.266392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 69.191232,
   "end_time": "2024-05-09T20:23:11.684837",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-09T20:22:02.493605",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
