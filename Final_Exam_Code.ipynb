{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":69.191232,"end_time":"2024-05-09T20:23:11.684837","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-09T20:22:02.493605","version":"2.5.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Final Exam Machine Learning","metadata":{"papermill":{"duration":0.004634,"end_time":"2024-05-09T20:22:05.309483","exception":false,"start_time":"2024-05-09T20:22:05.304849","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Davide Tateo; 167275\n\nFrancesca Salute; 167284\n\nNicole Favero; 167340 \n\nTomás Gonçalves; 167288","metadata":{"papermill":{"duration":0.003633,"end_time":"2024-05-09T20:22:05.317362","exception":false,"start_time":"2024-05-09T20:22:05.313729","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!git clone https://github.com/frasalute/ML_FinalExam.git","metadata":{"papermill":{"duration":25.36528,"end_time":"2024-05-09T20:22:30.686398","exception":false,"start_time":"2024-05-09T20:22:05.321118","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T18:08:40.948741Z","iopub.execute_input":"2024-05-10T18:08:40.949064Z","iopub.status.idle":"2024-05-10T18:09:09.501056Z","shell.execute_reply.started":"2024-05-10T18:08:40.949036Z","shell.execute_reply":"2024-05-10T18:09:09.499950Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'ML_FinalExam'...\nremote: Enumerating objects: 22346, done.\u001b[K\nremote: Counting objects: 100% (8158/8158), done.\u001b[K\nremote: Compressing objects: 100% (8143/8143), done.\u001b[K\nremote: Total 22346 (delta 31), reused 8132 (delta 14), pack-reused 14188\u001b[K\nReceiving objects: 100% (22346/22346), 401.91 MiB | 27.38 MiB/s, done.\nResolving deltas: 100% (42/42), done.\nUpdating files: 100% (22257/22257), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nprint(f\"Current working directory: {os.getcwd()}\")\nprint(f\"Contents of the current directory: {os.listdir('.')}\")","metadata":{"papermill":{"duration":0.02752,"end_time":"2024-05-09T20:22:30.733003","exception":false,"start_time":"2024-05-09T20:22:30.705483","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T18:09:09.503084Z","iopub.execute_input":"2024-05-10T18:09:09.503406Z","iopub.status.idle":"2024-05-10T18:09:09.509111Z","shell.execute_reply.started":"2024-05-10T18:09:09.503378Z","shell.execute_reply":"2024-05-10T18:09:09.508194Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Current working directory: /kaggle/working\nContents of the current directory: ['ML_FinalExam', '.virtual_documents']\n","output_type":"stream"}]},{"cell_type":"code","source":"base_path = '/kaggle/working/ML_FinalExam/images'\nprint(\"Base path is:\", base_path)","metadata":{"papermill":{"duration":0.037028,"end_time":"2024-05-09T20:22:30.788959","exception":false,"start_time":"2024-05-09T20:22:30.751931","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T18:09:09.510247Z","iopub.execute_input":"2024-05-10T18:09:09.510546Z","iopub.status.idle":"2024-05-10T18:09:09.519289Z","shell.execute_reply.started":"2024-05-10T18:09:09.510523Z","shell.execute_reply":"2024-05-10T18:09:09.518454Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Base path is: /kaggle/working/ML_FinalExam/images\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        # Currently, memory growth needs to be the same across GPUs\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        # Memory growth must be set before GPUs have been initialized\n        print(e)","metadata":{"papermill":{"duration":13.163557,"end_time":"2024-05-09T20:22:43.976467","exception":false,"start_time":"2024-05-09T20:22:30.812910","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T18:09:09.521163Z","iopub.execute_input":"2024-05-10T18:09:09.521490Z","iopub.status.idle":"2024-05-10T18:09:20.959571Z","shell.execute_reply.started":"2024-05-10T18:09:09.521449Z","shell.execute_reply":"2024-05-10T18:09:20.958687Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-05-10 18:09:11.097988: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-10 18:09:11.098083: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-10 18:09:11.217698: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Importing required API and modules\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.utils import img_to_array\nfrom keras.preprocessing.image import array_to_img\nfrom tensorflow.keras.utils import load_img\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","metadata":{"papermill":{"duration":0.030045,"end_time":"2024-05-09T20:22:44.024258","exception":false,"start_time":"2024-05-09T20:22:43.994213","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T18:09:20.960741Z","iopub.execute_input":"2024-05-10T18:09:20.961275Z","iopub.status.idle":"2024-05-10T18:09:20.969758Z","shell.execute_reply.started":"2024-05-10T18:09:20.961247Z","shell.execute_reply":"2024-05-10T18:09:20.968699Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def load_images_to_df(base_path, categories):\n    data = {'photo_id': [], 'image': [], 'image_array' :[], 'category': []}\n    categories = {'Benign': 0, 'Malignant': 1}\n    \n    for subset in ['train', 'test']:\n        for category in categories.keys():\n            folder_path = os.path.join(base_path, subset, category)\n            for filename in os.listdir(folder_path):\n                if filename.endswith('.jpg'):\n                    file_path = os.path.join(folder_path, filename)\n                    # Upload Images\n                    image = load_img(file_path)\n                    image_array = img_to_array(image)\n                    # Add the data\n                    data['photo_id'].append(filename)\n                    data['image'].append(image)\n                    data['image_array'].append(image_array)\n                    data['category'].append(categories[category])\n    \n    # Create DataFrame\n    df = pd.DataFrame(data)\n    return df\n\n# Define categories outside the function so it's easier to modify or append later on \ncategories = {'Benign': 0, 'Malignant': 1}\ndf = load_images_to_df(base_path, categories)","metadata":{"papermill":{"duration":16.952118,"end_time":"2024-05-09T20:23:00.994037","exception":false,"start_time":"2024-05-09T20:22:44.041919","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T18:09:20.971183Z","iopub.execute_input":"2024-05-10T18:09:20.971501Z","iopub.status.idle":"2024-05-10T18:09:37.961718Z","shell.execute_reply.started":"2024-05-10T18:09:20.971472Z","shell.execute_reply":"2024-05-10T18:09:37.960866Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(df.head())\nrow_count = len(df)\nprint(\"Number of rows in the DataFrame:\", row_count)","metadata":{"papermill":{"duration":8.100521,"end_time":"2024-05-09T20:23:09.114178","exception":false,"start_time":"2024-05-09T20:23:01.013657","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T18:09:37.962823Z","iopub.execute_input":"2024-05-10T18:09:37.963115Z","iopub.status.idle":"2024-05-10T18:09:45.968376Z","shell.execute_reply.started":"2024-05-10T18:09:37.963091Z","shell.execute_reply":"2024-05-10T18:09:45.967399Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"   photo_id                                              image  \\\n0  6154.jpg  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n1  3582.jpg  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n2  2657.jpg  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n3     2.jpg  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n4  4941.jpg  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n\n                                         image_array  category  \n0  [[[170.0, 129.0, 145.0], [172.0, 131.0, 147.0]...         0  \n1  [[[140.0, 108.0, 87.0], [141.0, 109.0, 88.0], ...         0  \n2  [[[170.0, 130.0, 156.0], [174.0, 134.0, 160.0]...         0  \n3  [[[253.0, 202.0, 245.0], [255.0, 205.0, 248.0]...         0  \n4  [[[182.0, 144.0, 155.0], [183.0, 145.0, 156.0]...         0  \nNumber of rows in the DataFrame: 13879\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Code to make augmentation of pics","metadata":{"papermill":{"duration":0.016977,"end_time":"2024-05-09T20:23:09.148992","exception":false,"start_time":"2024-05-09T20:23:09.132015","status":"completed"},"tags":[]}},{"cell_type":"code","source":"'''original_dataset_dir = './images/undetec_to_augm'\ntrain_augm_undetected_dir = './images/train/Undetected'\ntest_augm_undetected_dir ='./images/test/Undetected'\n\n# Create a data generator for augmentation\ndatagen = ImageDataGenerator(\n    rotation_range=30,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    brightness_range = [0.8, 1.3],\n    fill_mode='nearest'\n)\n\n# List the files in the original dataset directory\nfile_list = os.listdir(original_dataset_dir)\n\n# Ensure the target directories exist\nos.makedirs(train_augm_undetected_dir, exist_ok=True)\nos.makedirs(test_augm_undetected_dir, exist_ok=True)\n\n# Desired number of images after augmentation\ntarget_count_1 = 6000\ntarget_count_2 = 1000\n\"\"\"= target_count_2 = target_count_3 = target_count_4 = target_count_5 = target_count_6 = target_count_7\"\"\"\n\n# Number of images in the original dataset\noriginal_count = len(file_list)\n\n\n# Number of times to repeat each image to reach the target count\nrepeats_1 = min(target_count_1 // original_count + 1, len(file_list))\nrepeats_2 = min(target_count_2 // original_count + 1, len(file_list))\n\"\"\"repeats_3 = min(target_count_3 // original_count + 1, len(file_list))\nrepeats_4 = min(target_count_4 // original_count + 1, len(file_list))\nrepeats_5 = min(target_count_5 // original_count + 1, len(file_list))\nrepeats_6 = min(target_count_6 // original_count + 1, len(file_list))\nrepeats_7 = min(target_count_7 // original_count + 1, len(file_list))\"\"\"\n\n# Augment and save the images for the 6000 train images\nfor file in file_list[:original_count]:\n    img_path = os.path.join(original_dataset_dir, file)\n    img = load_img(img_path)\n    img = img_to_array(img)\n    img = img.reshape((1,) + img.shape)\n\n    i = 0\n    for batch in datagen.flow(img, batch_size=1, save_to_dir = train_augm_undetected_dir, save_prefix='augm', save_format='jpeg'): \n        #if in the previous line i save them as jpg, it anyway augments only the jpeg ones\n        i += 1\n        if i >= repeats_1:\n            break  # break the loop after reaching the desired number of augmented images\n\n\n\n# Augment and save the images for the 1000 test images\nfor file in file_list[:original_count]:\n    img_path = os.path.join(original_dataset_dir, file)\n    img = load_img(img_path)\n    img = img_to_array(img)\n    img = img.reshape((1,) + img.shape)\n\n    i = 0\n    for batch in datagen.flow(img, batch_size=1, save_to_dir = test_augm_undetected_dir, save_prefix='aug', save_format='jpeg'):\n        i += 1\n        if i >= repeats_2:\n            break  # break the loop after reaching the desired number of augmented images '''","metadata":{"papermill":{"duration":0.035787,"end_time":"2024-05-09T20:23:09.204724","exception":false,"start_time":"2024-05-09T20:23:09.168937","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T18:09:45.969685Z","iopub.execute_input":"2024-05-10T18:09:45.970020Z","iopub.status.idle":"2024-05-10T18:09:45.980497Z","shell.execute_reply.started":"2024-05-10T18:09:45.969993Z","shell.execute_reply":"2024-05-10T18:09:45.979563Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'original_dataset_dir = \\'./images/undetec_to_augm\\'\\ntrain_augm_undetected_dir = \\'./images/train/Undetected\\'\\ntest_augm_undetected_dir =\\'./images/test/Undetected\\'\\n\\n# Create a data generator for augmentation\\ndatagen = ImageDataGenerator(\\n    rotation_range=30,\\n    width_shift_range=0.2,\\n    height_shift_range=0.2,\\n    shear_range=0.2,\\n    zoom_range=0.2,\\n    horizontal_flip=True,\\n    brightness_range = [0.8, 1.3],\\n    fill_mode=\\'nearest\\'\\n)\\n\\n# List the files in the original dataset directory\\nfile_list = os.listdir(original_dataset_dir)\\n\\n# Ensure the target directories exist\\nos.makedirs(train_augm_undetected_dir, exist_ok=True)\\nos.makedirs(test_augm_undetected_dir, exist_ok=True)\\n\\n# Desired number of images after augmentation\\ntarget_count_1 = 6000\\ntarget_count_2 = 1000\\n\"\"\"= target_count_2 = target_count_3 = target_count_4 = target_count_5 = target_count_6 = target_count_7\"\"\"\\n\\n# Number of images in the original dataset\\noriginal_count = len(file_list)\\n\\n\\n# Number of times to repeat each image to reach the target count\\nrepeats_1 = min(target_count_1 // original_count + 1, len(file_list))\\nrepeats_2 = min(target_count_2 // original_count + 1, len(file_list))\\n\"\"\"repeats_3 = min(target_count_3 // original_count + 1, len(file_list))\\nrepeats_4 = min(target_count_4 // original_count + 1, len(file_list))\\nrepeats_5 = min(target_count_5 // original_count + 1, len(file_list))\\nrepeats_6 = min(target_count_6 // original_count + 1, len(file_list))\\nrepeats_7 = min(target_count_7 // original_count + 1, len(file_list))\"\"\"\\n\\n# Augment and save the images for the 6000 train images\\nfor file in file_list[:original_count]:\\n    img_path = os.path.join(original_dataset_dir, file)\\n    img = load_img(img_path)\\n    img = img_to_array(img)\\n    img = img.reshape((1,) + img.shape)\\n\\n    i = 0\\n    for batch in datagen.flow(img, batch_size=1, save_to_dir = train_augm_undetected_dir, save_prefix=\\'augm\\', save_format=\\'jpeg\\'): \\n        #if in the previous line i save them as jpg, it anyway augments only the jpeg ones\\n        i += 1\\n        if i >= repeats_1:\\n            break  # break the loop after reaching the desired number of augmented images\\n\\n\\n\\n# Augment and save the images for the 1000 test images\\nfor file in file_list[:original_count]:\\n    img_path = os.path.join(original_dataset_dir, file)\\n    img = load_img(img_path)\\n    img = img_to_array(img)\\n    img = img.reshape((1,) + img.shape)\\n\\n    i = 0\\n    for batch in datagen.flow(img, batch_size=1, save_to_dir = test_augm_undetected_dir, save_prefix=\\'aug\\', save_format=\\'jpeg\\'):\\n        i += 1\\n        if i >= repeats_2:\\n            break  # break the loop after reaching the desired number of augmented images '"},"metadata":{}}]},{"cell_type":"code","source":"# Append the new category to the dictionary of possible categories\ncategories['Undetected'] = 2","metadata":{"execution":{"iopub.status.busy":"2024-05-10T18:09:45.981838Z","iopub.execute_input":"2024-05-10T18:09:45.982171Z","iopub.status.idle":"2024-05-10T18:09:45.993131Z","shell.execute_reply.started":"2024-05-10T18:09:45.982126Z","shell.execute_reply":"2024-05-10T18:09:45.992366Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2024-05-10T18:09:45.995568Z","iopub.execute_input":"2024-05-10T18:09:45.995871Z","iopub.status.idle":"2024-05-10T18:09:46.846707Z","shell.execute_reply.started":"2024-05-10T18:09:45.995848Z","shell.execute_reply":"2024-05-10T18:09:46.845757Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Making sure it's a 2D array\nimage_matrix = np.stack(df['image_array'].values)\nprint(image_matrix)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T18:09:46.847987Z","iopub.execute_input":"2024-05-10T18:09:46.848363Z","iopub.status.idle":"2024-05-10T18:09:49.275234Z","shell.execute_reply.started":"2024-05-10T18:09:46.848332Z","shell.execute_reply":"2024-05-10T18:09:49.274327Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[[[[170. 129. 145.]\n   [172. 131. 147.]\n   [170. 129. 145.]\n   ...\n   [202. 167. 174.]\n   [200. 165. 172.]\n   [199. 164. 171.]]\n\n  [[169. 128. 144.]\n   [171. 130. 146.]\n   [172. 131. 147.]\n   ...\n   [202. 167. 174.]\n   [200. 165. 172.]\n   [199. 164. 171.]]\n\n  [[170. 129. 145.]\n   [171. 130. 146.]\n   [173. 132. 146.]\n   ...\n   [202. 167. 174.]\n   [201. 166. 173.]\n   [199. 164. 171.]]\n\n  ...\n\n  [[176. 144. 149.]\n   [179. 147. 152.]\n   [183. 148. 154.]\n   ...\n   [195. 159. 163.]\n   [193. 157. 161.]\n   [191. 155. 159.]]\n\n  [[174. 142. 147.]\n   [177. 145. 150.]\n   [182. 147. 153.]\n   ...\n   [196. 160. 164.]\n   [193. 157. 161.]\n   [191. 155. 159.]]\n\n  [[173. 141. 146.]\n   [176. 144. 149.]\n   [181. 146. 152.]\n   ...\n   [195. 159. 163.]\n   [193. 157. 161.]\n   [190. 154. 158.]]]\n\n\n [[[140. 108.  87.]\n   [141. 109.  88.]\n   [143. 111.  90.]\n   ...\n   [133. 100.  85.]\n   [147. 114.  99.]\n   [137. 104.  89.]]\n\n  [[140. 108.  87.]\n   [141. 109.  88.]\n   [143. 111.  90.]\n   ...\n   [134. 101.  86.]\n   [147. 114.  99.]\n   [138. 105.  90.]]\n\n  [[140. 108.  87.]\n   [141. 109.  88.]\n   [143. 111.  90.]\n   ...\n   [133. 100.  85.]\n   [147. 114.  99.]\n   [138. 105.  90.]]\n\n  ...\n\n  [[130. 113.  87.]\n   [126. 109.  83.]\n   [127. 110.  84.]\n   ...\n   [143. 117. 100.]\n   [140. 114.  97.]\n   [136. 110.  93.]]\n\n  [[130. 114.  88.]\n   [125. 109.  83.]\n   [126. 109.  83.]\n   ...\n   [143. 117. 100.]\n   [140. 114.  97.]\n   [134. 108.  91.]]\n\n  [[130. 114.  88.]\n   [125. 109.  83.]\n   [126. 109.  83.]\n   ...\n   [143. 117. 100.]\n   [139. 113.  96.]\n   [134. 108.  91.]]]\n\n\n [[[170. 130. 156.]\n   [174. 134. 160.]\n   [180. 140. 166.]\n   ...\n   [172. 125. 145.]\n   [170. 125. 145.]\n   [172. 127. 147.]]\n\n  [[173. 133. 159.]\n   [176. 136. 162.]\n   [179. 139. 165.]\n   ...\n   [175. 128. 148.]\n   [177. 132. 152.]\n   [181. 136. 156.]]\n\n  [[176. 136. 162.]\n   [177. 137. 163.]\n   [178. 138. 164.]\n   ...\n   [180. 133. 153.]\n   [185. 140. 160.]\n   [190. 145. 165.]]\n\n  ...\n\n  [[162. 120. 140.]\n   [162. 120. 140.]\n   [160. 118. 138.]\n   ...\n   [170. 121. 140.]\n   [167. 120. 138.]\n   [167. 120. 138.]]\n\n  [[162. 120. 140.]\n   [162. 120. 140.]\n   [159. 117. 137.]\n   ...\n   [168. 119. 138.]\n   [167. 120. 138.]\n   [171. 124. 142.]]\n\n  [[162. 120. 140.]\n   [161. 119. 139.]\n   [158. 116. 136.]\n   ...\n   [167. 118. 137.]\n   [167. 120. 138.]\n   [173. 126. 144.]]]\n\n\n ...\n\n\n [[[  0.   0.   0.]\n   [  0.   0.   0.]\n   [  0.   0.   0.]\n   ...\n   [  0.   0.   0.]\n   [  0.   0.   0.]\n   [  0.   0.   0.]]\n\n  [[  0.   0.   0.]\n   [  0.   0.   0.]\n   [  0.   0.   0.]\n   ...\n   [  0.   0.   0.]\n   [  0.   0.   0.]\n   [  0.   0.   0.]]\n\n  [[  0.   0.   0.]\n   [  0.   0.   0.]\n   [  0.   0.   0.]\n   ...\n   [  0.   0.   0.]\n   [  0.   0.   0.]\n   [  0.   0.   0.]]\n\n  ...\n\n  [[  0.   0.   0.]\n   [  0.   0.   0.]\n   [  0.   0.   0.]\n   ...\n   [  1.   1.   1.]\n   [  1.   1.   1.]\n   [  1.   1.   1.]]\n\n  [[  0.   0.   0.]\n   [  0.   0.   0.]\n   [  0.   0.   0.]\n   ...\n   [  1.   1.   1.]\n   [  1.   1.   1.]\n   [  1.   1.   1.]]\n\n  [[  0.   0.   0.]\n   [  0.   0.   0.]\n   [  0.   0.   0.]\n   ...\n   [  1.   1.   1.]\n   [  1.   1.   1.]\n   [  1.   1.   1.]]]\n\n\n [[[158. 136. 138.]\n   [152. 131. 128.]\n   [151. 126. 121.]\n   ...\n   [145. 133. 121.]\n   [135. 122. 114.]\n   [131. 121. 112.]]\n\n  [[158. 136. 138.]\n   [153. 129. 127.]\n   [150. 125. 118.]\n   ...\n   [154. 140. 129.]\n   [134. 121. 113.]\n   [120. 110. 101.]]\n\n  [[159. 135. 135.]\n   [152. 128. 126.]\n   [148. 123. 116.]\n   ...\n   [161. 147. 136.]\n   [140. 127. 119.]\n   [121. 108. 100.]]\n\n  ...\n\n  [[163. 148. 145.]\n   [164. 146. 144.]\n   [164. 140. 138.]\n   ...\n   [145. 127. 115.]\n   [140. 122. 110.]\n   [139. 121. 109.]]\n\n  [[162. 146. 146.]\n   [164. 146. 144.]\n   [165. 141. 139.]\n   ...\n   [140. 122. 110.]\n   [138. 120. 108.]\n   [141. 123. 111.]]\n\n  [[161. 145. 145.]\n   [163. 145. 143.]\n   [166. 142. 142.]\n   ...\n   [137. 119. 107.]\n   [136. 118. 106.]\n   [143. 125. 113.]]]\n\n\n [[[ 46.  51.  57.]\n   [ 51.  54.  59.]\n   [ 77.  72.  76.]\n   ...\n   [117. 114. 109.]\n   [121. 121. 119.]\n   [121. 126. 122.]]\n\n  [[ 48.  53.  59.]\n   [ 59.  60.  65.]\n   [ 78.  73.  77.]\n   ...\n   [129. 124. 120.]\n   [133. 134. 129.]\n   [124. 129. 125.]]\n\n  [[ 53.  56.  61.]\n   [ 61.  62.  66.]\n   [ 70.  66.  67.]\n   ...\n   [130. 125. 121.]\n   [134. 133. 129.]\n   [121. 123. 118.]]\n\n  ...\n\n  [[193. 194. 189.]\n   [192. 193. 188.]\n   [188. 187. 182.]\n   ...\n   [ 57.  53.  54.]\n   [ 77.  66.  72.]\n   [120. 107. 114.]]\n\n  [[191. 193. 188.]\n   [192. 193. 188.]\n   [188. 187. 183.]\n   ...\n   [ 63.  59.  60.]\n   [ 96.  85.  91.]\n   [105.  92. 101.]]\n\n  [[191. 193. 188.]\n   [189. 191. 186.]\n   [185. 184. 180.]\n   ...\n   [ 78.  76.  77.]\n   [121. 110. 116.]\n   [100.  87.  96.]]]]\n","output_type":"stream"}]},{"cell_type":"code","source":"# It's a three dimensions array so we have to flatten it \nimage_matrix = np.array([img.flatten() for img in df['image_array']])\n\n# Scale the data\nscaler = StandardScaler()\nimage_matrix_scaled = scaler.fit_transform(image_matrix)\n\n# Using cumulative variance ratio\npca_ratio=PCA()\npca_ratio.fit(image_matrix_scaled) # fit the PCA so it can learn\n\n# Two ways to proceed to obtain the number of components\n\n# Using cumulative variance\ncumulative_variance_ratio=np.cumsum(pca_ratio.explained_variance_ratio_)\nvariance=0.95 # set to 95% to keep a sufficiently large portion of the variance\nn_components= np.argmax(cumulative_variance_ratio >= variance) +1 # find the number of components needed \n\nprint(f\"Number of principal components: {n_components}\")\n\n# Setting threshold using pca\npca_threshold = PCA(n_components=0.99)\npca_threshold.fit(image_matrix_scaled) # fit the pca model to the data to learn patterns\nprint(f\"Number of principal components: {pca_threshold.n_components_}\")\n\n# Screen plot eigenvalues - number of principal components\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=np.arange(1, len(cumulative_variance_ratio) + 1), y=cumulative_variance_ratio, marker='o', color='#FF69B4')\nplt.title('Scree Plot')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance Ratio')\nplt.grid(True)\nplt.show()","metadata":{"papermill":{"duration":0.020456,"end_time":"2024-05-09T20:23:09.245954","exception":false,"start_time":"2024-05-09T20:23:09.225498","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T18:09:49.276554Z","iopub.execute_input":"2024-05-10T18:09:49.276916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transform the original data using retained principal components \ndf_reduced= pca_threshold.transform(image_matrix_scaled) # transform the original data to reduced dimensionality","metadata":{"papermill":{"duration":0.020219,"end_time":"2024-05-09T20:23:09.286611","exception":false,"start_time":"2024-05-09T20:23:09.266392","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}