{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO NOT RUN UNTIL FINALISED  \n",
    "## CNN SKELETON \n",
    "some variable names are to be changed depending on original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTING LIBRARIES\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINING TRAIN AND TEST DATASETS, AND VISUALISING SOME IMAGES WITH THEIR LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make this notebookâ€™s output stable across runs\n",
    "np.random.seed(50)\n",
    "tf.random.set_seed(50)\n",
    "\n",
    "# Load the Dataset  -  I am calling it full in case we want to cut down the train images to 6250 for all train categories\n",
    "(X_train_full, y_train_full)  = # insert train datasets X_train is the list of images, y_train is the list of labels\n",
    "(X_test, y_test) = # insert same thing but for test datasets\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "X_train_full = X_train_full.astype(np.float32) / 255\n",
    "X_test = X_test.astype(np.float32) / 255\n",
    "\n",
    "\"\"\" THIS MAY BE USEFUL DEPENDING ON HOW WE LOAD THE IMAGES, WETHER AS ONLY ONE BIG TABLE OR ALREADY DIVIDED INTO TEST AND TRAIN\n",
    "# Split the full training set into a training set and a (smaller) test set\n",
    "# The first 6250 images and labels will be used as the training set\n",
    "X_train, y_train = X_train_full[6250:], y_train_full[6250:]\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#THE FOLLOWING CODE NEEDS TO BE HEAVILY REWRITTEN DEPENDING ON HOW WE STRUCTURE OUR TABLE CONATINING THE IMAGES!!!!!!!\n",
    "#THE FOLLOWING CODE NEEDS TO BE HEAVILY REWRITTEN DEPENDING ON HOW WE STRUCTURE OUR TABLE CONATINING THE IMAGES!!!!!!!\n",
    "#THE FOLLOWING CODE NEEDS TO BE HEAVILY REWRITTEN DEPENDING ON HOW WE STRUCTURE OUR TABLE CONATINING THE IMAGES!!!!!!!\n",
    "category = ['Benign', 'Malignant', 'Undetected']\n",
    "\n",
    "# visualising images with numeric tags and names  \n",
    "def display_images_with_labels(images, labels, category, num_images=10, tagnum = None):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    # select only images with specified tag number\n",
    "    if tagnum >=0 and tagnum<3: \n",
    "        indices = np.where(np.array([str(label) for label in labels]) == str(tagnum))[0][:num_images]\n",
    "    # otherwise select randomly\n",
    "    else: \n",
    "        indices = np.random.choice(range(len(images)), num_images, replace=False)\n",
    "    for i, index in enumerate(indices):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(images[index], cmap=plt.cm.binary)\n",
    "         # if there is a list of class names, show both tag number and class name\n",
    "        if class_names:\n",
    "            label_with_name = f\"{labels[index]} ({class_names[labels[index]]})\"\n",
    "            plt.xlabel(label_with_name)\n",
    "        # othwerwise visualise only tag number\n",
    "        else:\n",
    "            plt.xlabel(labels[index])\n",
    "    plt.show()\n",
    "\n",
    "display_images_with_labels(X_train, y_train, class_names, num_images=25, category = 0)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATING THE MODEL AND INITIALISING DATASETS\n",
    "## MODEL NEEDS TO BE MODIFIED DEPENDING ON THIS SPECIFIC CASE\n",
    "This code is just an example of layers, the final composition of the model is to be decided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the models\n",
    "# Lower Model\n",
    "lower_model = keras.Sequential()\n",
    "lower_model.add(keras.Input(shape=(28, 28, 1)))  # 28x28 B&W\n",
    "\n",
    "lower_model.add(layers.Conv2D(32, 3, activation=\"relu\", padding='same', kernel_initializer='he_normal'))\n",
    "lower_model.add(layers.MaxPooling2D(2))\n",
    "\n",
    "lower_model.add(layers.Conv2D(64, 3, activation=\"relu\", padding='same', kernel_initializer='he_normal'))\n",
    "lower_model.add(layers.MaxPooling2D(2))\n",
    "\n",
    "# Upper Model\n",
    "upper_model = keras.Sequential()\n",
    "upper_model.add(layers.Flatten()) \n",
    "\n",
    "upper_model.add(layers.Dense(128, activation='relu', kernel_initializer='he_normal'))\n",
    "upper_model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Prepare the training dataset.\n",
    "batch_size = 32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(1024).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)\n",
    "\n",
    "# Optimizer for the lower layers\n",
    "lower_optimizer = keras.optimizers.SGD(learning_rate=1e-4)\n",
    "# Optimizer for the upper layers\n",
    "upper_optimizer = keras.optimizers.Nadam(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Prepare the testing dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INITIALISING METRICS AND RUNNING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics at the start of each epoch\n",
    "loss_avg = keras.metrics.Mean()\n",
    "accuracy = keras.metrics.SparseCategoricalAccuracy()\n",
    "val_loss_avg = keras.metrics.Mean()\n",
    "val_accuracy = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# Training Loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nStart of epoch {epoch}\")\n",
    "\n",
    "    # Iterate over the batches of the training dataset\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        # Calculating the intermediate activations of the lower and upper model \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            lower_logits = lower_model(x_batch_train, training=True)\n",
    "            logits = upper_model(lower_logits, training=True)\n",
    "        # Calcutating the loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "        # Gradient computation for both the models\n",
    "        lower_grads = tape.gradient(loss_value, lower_model.trainable_weights)\n",
    "        upper_grads = tape.gradient(loss_value, upper_model.trainable_weights)\n",
    "        \n",
    "        # Apply optimizers\n",
    "        lower_optimizer.apply_gradients(zip(lower_grads, lower_model.trainable_weights))\n",
    "        upper_optimizer.apply_gradients(zip(upper_grads, upper_model.trainable_weights))\n",
    "\n",
    "        # Update training metrics\n",
    "        loss_avg.update_state(loss_value)\n",
    "        accuracy.update_state(y_batch_train, logits)\n",
    "\n",
    "    # Print the mean training loss and accuracy over the epoch\n",
    "    train_loss = loss_avg.result()\n",
    "    train_accuracy = accuracy.result()\n",
    "    print(f\"Training loss over epoch: {float(train_loss):.4f}\")\n",
    "    print(f\"Training accuracy over epoch: {float(train_accuracy):.4f}\")\n",
    "\n",
    "\n",
    "    # Perform validation at the end of the epoch\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        lower_val_logits = lower_model(x_batch_val, training=False)\n",
    "        val_logits = upper_model(lower_val_logits, training=False)\n",
    "        val_loss_value = loss_fn(y_batch_val, val_logits)\n",
    "    \n",
    "        val_loss_avg.update_state(val_loss_value)\n",
    "        val_accuracy.update_state(y_batch_val, val_logits)\n",
    "    \n",
    "    # Compute the mean validation loss and accuracy for the epoch\n",
    "    val_loss = val_loss_avg.result()\n",
    "    val_acc = val_accuracy.result()\n",
    "    print(f\"Validation loss: {float(val_loss):.4f}\")\n",
    "    print(f\"Validation accuracy: {float(val_acc):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
