{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":69.191232,"end_time":"2024-05-09T20:23:11.684837","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-09T20:22:02.493605","version":"2.5.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Final Exam Machine Learning","metadata":{"papermill":{"duration":0.004634,"end_time":"2024-05-09T20:22:05.309483","exception":false,"start_time":"2024-05-09T20:22:05.304849","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Davide Tateo; 167275\n\nFrancesca Salute; 167284\n\nNicole Favero; 167340 \n\nTomás Gonçalves; 167288","metadata":{"papermill":{"duration":0.003633,"end_time":"2024-05-09T20:22:05.317362","exception":false,"start_time":"2024-05-09T20:22:05.313729","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!git clone https://github.com/frasalute/ML_FinalExam.git","metadata":{"papermill":{"duration":25.36528,"end_time":"2024-05-09T20:22:30.686398","exception":false,"start_time":"2024-05-09T20:22:05.321118","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T17:40:16.465343Z","iopub.execute_input":"2024-05-10T17:40:16.466449Z","iopub.status.idle":"2024-05-10T17:40:39.432406Z","shell.execute_reply.started":"2024-05-10T17:40:16.466413Z","shell.execute_reply":"2024-05-10T17:40:39.431283Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Cloning into 'ML_FinalExam'...\nremote: Enumerating objects: 22340, done.\u001b[K\nremote: Counting objects: 100% (8152/8152), done.\u001b[K\nremote: Compressing objects: 100% (8137/8137), done.\u001b[K\nremote: Total 22340 (delta 27), reused 8132 (delta 14), pack-reused 14188\u001b[K\nReceiving objects: 100% (22340/22340), 401.91 MiB | 45.57 MiB/s, done.\nResolving deltas: 100% (38/38), done.\nUpdating files: 100% (22257/22257), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nprint(f\"Current working directory: {os.getcwd()}\")\nprint(f\"Contents of the current directory: {os.listdir('.')}\")","metadata":{"papermill":{"duration":0.02752,"end_time":"2024-05-09T20:22:30.733003","exception":false,"start_time":"2024-05-09T20:22:30.705483","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T17:40:39.434312Z","iopub.execute_input":"2024-05-10T17:40:39.434590Z","iopub.status.idle":"2024-05-10T17:40:39.440326Z","shell.execute_reply.started":"2024-05-10T17:40:39.434563Z","shell.execute_reply":"2024-05-10T17:40:39.439411Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Current working directory: /kaggle/working\nContents of the current directory: ['ML_FinalExam', '.virtual_documents']\n","output_type":"stream"}]},{"cell_type":"code","source":"base_path = '/kaggle/working/ML_FinalExam/images'\nprint(\"Base path is:\", base_path)","metadata":{"papermill":{"duration":0.037028,"end_time":"2024-05-09T20:22:30.788959","exception":false,"start_time":"2024-05-09T20:22:30.751931","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T17:40:39.441333Z","iopub.execute_input":"2024-05-10T17:40:39.441613Z","iopub.status.idle":"2024-05-10T17:40:39.450075Z","shell.execute_reply.started":"2024-05-10T17:40:39.441581Z","shell.execute_reply":"2024-05-10T17:40:39.449121Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Base path is: /kaggle/working/ML_FinalExam/images\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        # Currently, memory growth needs to be the same across GPUs\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        # Memory growth must be set before GPUs have been initialized\n        print(e)","metadata":{"papermill":{"duration":13.163557,"end_time":"2024-05-09T20:22:43.976467","exception":false,"start_time":"2024-05-09T20:22:30.812910","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T17:40:39.452504Z","iopub.execute_input":"2024-05-10T17:40:39.453045Z","iopub.status.idle":"2024-05-10T17:40:50.670187Z","shell.execute_reply.started":"2024-05-10T17:40:39.453011Z","shell.execute_reply":"2024-05-10T17:40:50.669195Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"2024-05-10 17:40:41.042672: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-10 17:40:41.042792: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-10 17:40:41.169514: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Importing required API and modules\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.utils import img_to_array\nfrom keras.preprocessing.image import array_to_img\nfrom tensorflow.keras.utils import load_img\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","metadata":{"papermill":{"duration":0.030045,"end_time":"2024-05-09T20:22:44.024258","exception":false,"start_time":"2024-05-09T20:22:43.994213","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T17:40:50.671455Z","iopub.execute_input":"2024-05-10T17:40:50.672044Z","iopub.status.idle":"2024-05-10T17:40:50.680240Z","shell.execute_reply.started":"2024-05-10T17:40:50.672016Z","shell.execute_reply":"2024-05-10T17:40:50.679411Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def load_images_to_df(base_path, categories):\n    data = {'photo_id': [], 'image': [], 'image_array' :[], 'category': []}\n    categories = {'Benign': 0, 'Malignant': 1}\n    \n    for subset in ['train', 'test']:\n        for category in categories.keys():\n            folder_path = os.path.join(base_path, subset, category)\n            for filename in os.listdir(folder_path):\n                if filename.endswith('.jpg'):\n                    file_path = os.path.join(folder_path, filename)\n                    # Upload Images\n                    image = load_img(file_path)\n                    image_array = img_to_array(image)\n                    # Add the data\n                    data['photo_id'].append(filename)\n                    data['image'].append(image)\n                    data['image_array'].append(image_array)\n                    data['category'].append(categories[category])\n    \n    # Create DataFrame\n    df = pd.DataFrame(data)\n    return df\n\n# Define categories outside the function so it's easier to modify or append later on \ncategories = {'Benign': 0, 'Malignant': 1}\ndf = load_images_to_df(base_path, categories)","metadata":{"papermill":{"duration":16.952118,"end_time":"2024-05-09T20:23:00.994037","exception":false,"start_time":"2024-05-09T20:22:44.041919","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T17:40:50.682361Z","iopub.execute_input":"2024-05-10T17:40:50.682625Z","iopub.status.idle":"2024-05-10T17:41:07.414167Z","shell.execute_reply.started":"2024-05-10T17:40:50.682602Z","shell.execute_reply":"2024-05-10T17:41:07.413088Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"print(df.head())\nrow_count = len(df)\nprint(\"Number of rows in the DataFrame:\", row_count)","metadata":{"papermill":{"duration":8.100521,"end_time":"2024-05-09T20:23:09.114178","exception":false,"start_time":"2024-05-09T20:23:01.013657","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T17:41:07.415505Z","iopub.execute_input":"2024-05-10T17:41:07.415859Z","iopub.status.idle":"2024-05-10T17:41:15.772432Z","shell.execute_reply.started":"2024-05-10T17:41:07.415829Z","shell.execute_reply":"2024-05-10T17:41:15.771517Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"   photo_id                                              image  \\\n0   742.jpg  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n1  6169.jpg  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n2   913.jpg  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n3  3900.jpg  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n4  3452.jpg  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n\n                                         image_array  category  \n0  [[[208.0, 123.0, 156.0], [208.0, 123.0, 156.0]...         0  \n1  [[[149.0, 113.0, 97.0], [149.0, 113.0, 97.0], ...         0  \n2  [[[7.0, 4.0, 0.0], [7.0, 4.0, 0.0], [10.0, 2.0...         0  \n3  [[[193.0, 156.0, 164.0], [195.0, 158.0, 166.0]...         0  \n4  [[[94.0, 92.0, 95.0], [104.0, 102.0, 103.0], [...         0  \nNumber of rows in the DataFrame: 13879\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Code to make augmentation of pics","metadata":{"papermill":{"duration":0.016977,"end_time":"2024-05-09T20:23:09.148992","exception":false,"start_time":"2024-05-09T20:23:09.132015","status":"completed"},"tags":[]}},{"cell_type":"code","source":"'''original_dataset_dir = './images/undetec_to_augm'\ntrain_augm_undetected_dir = './images/train/Undetected'\ntest_augm_undetected_dir ='./images/test/Undetected'\n\n# Create a data generator for augmentation\ndatagen = ImageDataGenerator(\n    rotation_range=30,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    brightness_range = [0.8, 1.3],\n    fill_mode='nearest'\n)\n\n# List the files in the original dataset directory\nfile_list = os.listdir(original_dataset_dir)\n\n# Ensure the target directories exist\nos.makedirs(train_augm_undetected_dir, exist_ok=True)\nos.makedirs(test_augm_undetected_dir, exist_ok=True)\n\n# Desired number of images after augmentation\ntarget_count_1 = 6000\ntarget_count_2 = 1000\n\"\"\"= target_count_2 = target_count_3 = target_count_4 = target_count_5 = target_count_6 = target_count_7\"\"\"\n\n# Number of images in the original dataset\noriginal_count = len(file_list)\n\n\n# Number of times to repeat each image to reach the target count\nrepeats_1 = min(target_count_1 // original_count + 1, len(file_list))\nrepeats_2 = min(target_count_2 // original_count + 1, len(file_list))\n\"\"\"repeats_3 = min(target_count_3 // original_count + 1, len(file_list))\nrepeats_4 = min(target_count_4 // original_count + 1, len(file_list))\nrepeats_5 = min(target_count_5 // original_count + 1, len(file_list))\nrepeats_6 = min(target_count_6 // original_count + 1, len(file_list))\nrepeats_7 = min(target_count_7 // original_count + 1, len(file_list))\"\"\"\n\n# Augment and save the images for the 6000 train images\nfor file in file_list[:original_count]:\n    img_path = os.path.join(original_dataset_dir, file)\n    img = load_img(img_path)\n    img = img_to_array(img)\n    img = img.reshape((1,) + img.shape)\n\n    i = 0\n    for batch in datagen.flow(img, batch_size=1, save_to_dir = train_augm_undetected_dir, save_prefix='augm', save_format='jpeg'): \n        #if in the previous line i save them as jpg, it anyway augments only the jpeg ones\n        i += 1\n        if i >= repeats_1:\n            break  # break the loop after reaching the desired number of augmented images\n\n\n\n# Augment and save the images for the 1000 test images\nfor file in file_list[:original_count]:\n    img_path = os.path.join(original_dataset_dir, file)\n    img = load_img(img_path)\n    img = img_to_array(img)\n    img = img.reshape((1,) + img.shape)\n\n    i = 0\n    for batch in datagen.flow(img, batch_size=1, save_to_dir = test_augm_undetected_dir, save_prefix='aug', save_format='jpeg'):\n        i += 1\n        if i >= repeats_2:\n            break  # break the loop after reaching the desired number of augmented images '''","metadata":{"papermill":{"duration":0.035787,"end_time":"2024-05-09T20:23:09.204724","exception":false,"start_time":"2024-05-09T20:23:09.168937","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T17:41:15.773892Z","iopub.execute_input":"2024-05-10T17:41:15.774556Z","iopub.status.idle":"2024-05-10T17:41:15.784673Z","shell.execute_reply.started":"2024-05-10T17:41:15.774516Z","shell.execute_reply":"2024-05-10T17:41:15.783817Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'original_dataset_dir = \\'./images/undetec_to_augm\\'\\ntrain_augm_undetected_dir = \\'./images/train/Undetected\\'\\ntest_augm_undetected_dir =\\'./images/test/Undetected\\'\\n\\n# Create a data generator for augmentation\\ndatagen = ImageDataGenerator(\\n    rotation_range=30,\\n    width_shift_range=0.2,\\n    height_shift_range=0.2,\\n    shear_range=0.2,\\n    zoom_range=0.2,\\n    horizontal_flip=True,\\n    brightness_range = [0.8, 1.3],\\n    fill_mode=\\'nearest\\'\\n)\\n\\n# List the files in the original dataset directory\\nfile_list = os.listdir(original_dataset_dir)\\n\\n# Ensure the target directories exist\\nos.makedirs(train_augm_undetected_dir, exist_ok=True)\\nos.makedirs(test_augm_undetected_dir, exist_ok=True)\\n\\n# Desired number of images after augmentation\\ntarget_count_1 = 6000\\ntarget_count_2 = 1000\\n\"\"\"= target_count_2 = target_count_3 = target_count_4 = target_count_5 = target_count_6 = target_count_7\"\"\"\\n\\n# Number of images in the original dataset\\noriginal_count = len(file_list)\\n\\n\\n# Number of times to repeat each image to reach the target count\\nrepeats_1 = min(target_count_1 // original_count + 1, len(file_list))\\nrepeats_2 = min(target_count_2 // original_count + 1, len(file_list))\\n\"\"\"repeats_3 = min(target_count_3 // original_count + 1, len(file_list))\\nrepeats_4 = min(target_count_4 // original_count + 1, len(file_list))\\nrepeats_5 = min(target_count_5 // original_count + 1, len(file_list))\\nrepeats_6 = min(target_count_6 // original_count + 1, len(file_list))\\nrepeats_7 = min(target_count_7 // original_count + 1, len(file_list))\"\"\"\\n\\n# Augment and save the images for the 6000 train images\\nfor file in file_list[:original_count]:\\n    img_path = os.path.join(original_dataset_dir, file)\\n    img = load_img(img_path)\\n    img = img_to_array(img)\\n    img = img.reshape((1,) + img.shape)\\n\\n    i = 0\\n    for batch in datagen.flow(img, batch_size=1, save_to_dir = train_augm_undetected_dir, save_prefix=\\'augm\\', save_format=\\'jpeg\\'): \\n        #if in the previous line i save them as jpg, it anyway augments only the jpeg ones\\n        i += 1\\n        if i >= repeats_1:\\n            break  # break the loop after reaching the desired number of augmented images\\n\\n\\n\\n# Augment and save the images for the 1000 test images\\nfor file in file_list[:original_count]:\\n    img_path = os.path.join(original_dataset_dir, file)\\n    img = load_img(img_path)\\n    img = img_to_array(img)\\n    img = img.reshape((1,) + img.shape)\\n\\n    i = 0\\n    for batch in datagen.flow(img, batch_size=1, save_to_dir = test_augm_undetected_dir, save_prefix=\\'aug\\', save_format=\\'jpeg\\'):\\n        i += 1\\n        if i >= repeats_2:\\n            break  # break the loop after reaching the desired number of augmented images '"},"metadata":{}}]},{"cell_type":"code","source":"# Append the new category to the dictionary of possible categories\ncategories['Undetected'] = 2","metadata":{"execution":{"iopub.status.busy":"2024-05-10T17:41:15.785588Z","iopub.execute_input":"2024-05-10T17:41:15.785877Z","iopub.status.idle":"2024-05-10T17:41:15.798530Z","shell.execute_reply.started":"2024-05-10T17:41:15.785851Z","shell.execute_reply":"2024-05-10T17:41:15.797717Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2024-05-10T17:41:15.801082Z","iopub.execute_input":"2024-05-10T17:41:15.801352Z","iopub.status.idle":"2024-05-10T17:41:15.808400Z","shell.execute_reply.started":"2024-05-10T17:41:15.801329Z","shell.execute_reply":"2024-05-10T17:41:15.807552Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Making sure it's a 2D array\nimage_matrix = np.stack(df['image_array'].values)\nprint(image_matrix)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T17:46:39.940301Z","iopub.execute_input":"2024-05-10T17:46:39.940989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_matrix = np.array([img.flatten() for img in df['image_array']])\n\n# Scale the data\nscaler = StandardScaler()\nimage_matrix_scaled = scaler.fit_transform(image_matrix)\n\n# Using cumulative variance ratio\npca_ratio=PCA()\npca_ratio.fit(image_matrix_scaled) # fit the PCA so it can learn\n\n# Two ways to proceed to obtain the number of components\n\n# Using cumulative variance\ncumulative_variance_ratio=np.cumsum(pca_ratio.explained_variance_ratio_)\nvariance=0.95 # set to 95% to keep a sufficiently large portion of the variance\nn_components= np.argmax(cumulative_variance_ratio >= variance) +1 # find the number of components needed \n\nprint(f\"Number of principal components: {n_components}\")\n\n# Setting threshold using pca\npca_threshold = PCA(n_components=0.99)\npca_threshold.fit(image_matrix_scaled) # fit the pca model to the data to learn patterns\nprint(f\"Number of principal components: {pca_threshold.n_components_}\")\n\n# Screen plot eigenvalues - number of principal components\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=np.arange(1, len(cumulative_variance_ratio) + 1), y=cumulative_variance_ratio, marker='o', color='#FF69B4')\nplt.title('Scree Plot')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance Ratio')\nplt.grid(True)\nplt.show()","metadata":{"papermill":{"duration":0.020456,"end_time":"2024-05-09T20:23:09.245954","exception":false,"start_time":"2024-05-09T20:23:09.225498","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T17:41:15.809367Z","iopub.execute_input":"2024-05-10T17:41:15.809627Z","iopub.status.idle":"2024-05-10T17:41:19.075593Z","shell.execute_reply.started":"2024-05-10T17:41:15.809604Z","shell.execute_reply":"2024-05-10T17:41:19.073927Z"},"trusted":true},"execution_count":13,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Scale the data\u001b[39;00m\n\u001b[1;32m      6\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m----> 7\u001b[0m image_matrix_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Using cumulative variance ratio\u001b[39;00m\n\u001b[1;32m     10\u001b[0m pca_ratio\u001b[38;5;241m=\u001b[39mPCA()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:878\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:824\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 824\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:861\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m    860\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 861\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    868\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 565\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:915\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    912\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    913\u001b[0m     )\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m--> 915\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    918\u001b[0m     )\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m    921\u001b[0m     _assert_all_finite(\n\u001b[1;32m    922\u001b[0m         array,\n\u001b[1;32m    923\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    924\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    925\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    926\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: Found array with dim 4. StandardScaler expected <= 2."],"ename":"ValueError","evalue":"Found array with dim 4. StandardScaler expected <= 2.","output_type":"error"}]},{"cell_type":"code","source":"# After finding 260 principal components to preserve 99% variance\n# Transform the original data using retained principal components \n\ndf_reduced= pca_threshold.transform(df) # transform the original data to reduced dimensionality","metadata":{"papermill":{"duration":0.020219,"end_time":"2024-05-09T20:23:09.286611","exception":false,"start_time":"2024-05-09T20:23:09.266392","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-10T17:41:19.076800Z","iopub.status.idle":"2024-05-10T17:41:19.077910Z","shell.execute_reply.started":"2024-05-10T17:41:19.077591Z","shell.execute_reply":"2024-05-10T17:41:19.077618Z"},"trusted":true},"execution_count":null,"outputs":[]}]}