{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.004634,"end_time":"2024-05-09T20:22:05.309483","exception":false,"start_time":"2024-05-09T20:22:05.304849","status":"completed"},"tags":[]},"source":["## Final Exam Machine Learning"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.003633,"end_time":"2024-05-09T20:22:05.317362","exception":false,"start_time":"2024-05-09T20:22:05.313729","status":"completed"},"tags":[]},"source":["Davide Tateo; 167275\n","\n","Francesca Salute; 167284\n","\n","Nicole Favero; 167340 \n","\n","Tomás Gonçalves; 167288"]},{"cell_type":"code","execution_count":1,"id":"c01b779b","metadata":{"execution":{"iopub.execute_input":"2024-05-10T18:08:40.949064Z","iopub.status.busy":"2024-05-10T18:08:40.948741Z","iopub.status.idle":"2024-05-10T18:09:09.501056Z","shell.execute_reply":"2024-05-10T18:09:09.499950Z","shell.execute_reply.started":"2024-05-10T18:08:40.949036Z"},"papermill":{"duration":25.36528,"end_time":"2024-05-09T20:22:30.686398","exception":false,"start_time":"2024-05-09T20:22:05.321118","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'ML_FinalExam'...\n","remote: Enumerating objects: 22346, done.\u001b[K\n","remote: Counting objects: 100% (8158/8158), done.\u001b[K\n","remote: Compressing objects: 100% (8143/8143), done.\u001b[K\n","remote: Total 22346 (delta 31), reused 8132 (delta 14), pack-reused 14188\u001b[K\n","Receiving objects: 100% (22346/22346), 401.91 MiB | 27.38 MiB/s, done.\n","Resolving deltas: 100% (42/42), done.\n","Updating files: 100% (22257/22257), done.\n"]}],"source":["#!git clone https://github.com/frasalute/ML_FinalExam.git"]},{"cell_type":"code","execution_count":1,"id":"7a3b910d","metadata":{"execution":{"iopub.execute_input":"2024-05-10T18:09:09.503406Z","iopub.status.busy":"2024-05-10T18:09:09.503084Z","iopub.status.idle":"2024-05-10T18:09:09.509111Z","shell.execute_reply":"2024-05-10T18:09:09.508194Z","shell.execute_reply.started":"2024-05-10T18:09:09.503378Z"},"papermill":{"duration":0.02752,"end_time":"2024-05-09T20:22:30.733003","exception":false,"start_time":"2024-05-09T20:22:30.705483","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Current working directory: c:\\Users\\davit\\Desktop\\CBS\\MACHINE LEARNING\\GithubExam\\ML_FinalExam\n","Contents of the current directory: ['.DS_Store', '.git', 'Final_Exam_Code.ipynb', 'images', 'LICENSE', 'README.md']\n"]}],"source":["import os\n","print(f\"Current working directory: {os.getcwd()}\")\n","print(f\"Contents of the current directory: {os.listdir('.')}\")"]},{"cell_type":"code","execution_count":3,"id":"16362855","metadata":{"execution":{"iopub.execute_input":"2024-05-10T18:09:09.510546Z","iopub.status.busy":"2024-05-10T18:09:09.510247Z","iopub.status.idle":"2024-05-10T18:09:09.519289Z","shell.execute_reply":"2024-05-10T18:09:09.518454Z","shell.execute_reply.started":"2024-05-10T18:09:09.510523Z"},"papermill":{"duration":0.037028,"end_time":"2024-05-09T20:22:30.788959","exception":false,"start_time":"2024-05-09T20:22:30.751931","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Base path is: /kaggle/working/ML_FinalExam/images\n"]}],"source":["base_path = '/work/ML_FinalExam/images'\n","print(\"Base path is:\", base_path)"]},{"cell_type":"code","execution_count":null,"id":"663cd4c7","metadata":{},"outputs":[],"source":["# pip install tensorflow\n","# pip install scikit-learn\n","# pip install seaborn"]},{"cell_type":"code","execution_count":4,"id":"c40083af","metadata":{"execution":{"iopub.execute_input":"2024-05-10T18:09:09.521490Z","iopub.status.busy":"2024-05-10T18:09:09.521163Z","iopub.status.idle":"2024-05-10T18:09:20.959571Z","shell.execute_reply":"2024-05-10T18:09:20.958687Z","shell.execute_reply.started":"2024-05-10T18:09:09.521449Z"},"papermill":{"duration":13.163557,"end_time":"2024-05-09T20:22:43.976467","exception":false,"start_time":"2024-05-09T20:22:30.812910","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-10 18:09:11.097988: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-10 18:09:11.098083: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-10 18:09:11.217698: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import tensorflow as tf\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        # Currently, memory growth needs to be the same across GPUs\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","    except RuntimeError as e:\n","        # Memory growth must be set before GPUs have been initialized\n","        print(e)"]},{"cell_type":"code","execution_count":5,"id":"8ada059f","metadata":{"execution":{"iopub.execute_input":"2024-05-10T18:09:20.961275Z","iopub.status.busy":"2024-05-10T18:09:20.960741Z","iopub.status.idle":"2024-05-10T18:09:20.969758Z","shell.execute_reply":"2024-05-10T18:09:20.968699Z","shell.execute_reply.started":"2024-05-10T18:09:20.961247Z"},"papermill":{"duration":0.030045,"end_time":"2024-05-09T20:22:44.024258","exception":false,"start_time":"2024-05-09T20:22:43.994213","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Importing required API and modules\n","import warnings\n","import numpy as np\n","import pandas as pd\n","from tensorflow.keras.utils import img_to_array\n","from keras.preprocessing.image import array_to_img\n","from tensorflow.keras.utils import load_img\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"code","execution_count":6,"id":"9f3f07cf","metadata":{"execution":{"iopub.execute_input":"2024-05-10T18:09:20.971501Z","iopub.status.busy":"2024-05-10T18:09:20.971183Z","iopub.status.idle":"2024-05-10T18:09:37.961718Z","shell.execute_reply":"2024-05-10T18:09:37.960866Z","shell.execute_reply.started":"2024-05-10T18:09:20.971472Z"},"papermill":{"duration":16.952118,"end_time":"2024-05-09T20:23:00.994037","exception":false,"start_time":"2024-05-09T20:22:44.041919","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def load_and_resize_images(base_path, categories, target_size=(128, 128)):\n","    data = {'photo_id': [], 'image': [], 'image_array': [], 'category': [], 'subset': []}\n","    \n","    for subset in ['train', 'test']:\n","        for category in categories.keys():\n","            folder_path = os.path.join(base_path, subset, category)\n","            for filename in os.listdir(folder_path):\n","                if filename.lower().endswith(('.jpg', '.jpeg')):  # Check for both .jpg and .jpeg extensions\n","                    file_path = os.path.join(folder_path, filename)\n","                    # Load and resize the image\n","                    image = load_img(file_path, target_size=target_size)\n","                    image_array = img_to_array(image)\n","                    # Add data to the DataFrame\n","                    data['photo_id'].append(filename)\n","                    data['image'].append(image)\n","                    data['image_array'].append(image_array)\n","                    data['category'].append(categories[category])\n","                    data['subset'].append(subset)\n","    \n","    df = pd.DataFrame(data)\n","    return df\n","\n","# Load the DataFrame\n","categories = {'Benign': 0, 'Malignant': 1, 'Undetected': 2}\n","base_path = '/work/ML_FinalExam/images'\n","df = load_and_resize_images(base_path,categories)"]},{"cell_type":"code","execution_count":7,"id":"527f6db1","metadata":{"execution":{"iopub.execute_input":"2024-05-10T18:09:37.963115Z","iopub.status.busy":"2024-05-10T18:09:37.962823Z","iopub.status.idle":"2024-05-10T18:09:45.968376Z","shell.execute_reply":"2024-05-10T18:09:45.967399Z","shell.execute_reply.started":"2024-05-10T18:09:37.963091Z"},"papermill":{"duration":8.100521,"end_time":"2024-05-09T20:23:09.114178","exception":false,"start_time":"2024-05-09T20:23:01.013657","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["   photo_id                                              image  \\\n","0  6154.jpg  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n","1  3582.jpg  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n","2  2657.jpg  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n","3     2.jpg  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n","4  4941.jpg  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n","\n","                                         image_array  category  \n","0  [[[170.0, 129.0, 145.0], [172.0, 131.0, 147.0]...         0  \n","1  [[[140.0, 108.0, 87.0], [141.0, 109.0, 88.0], ...         0  \n","2  [[[170.0, 130.0, 156.0], [174.0, 134.0, 160.0]...         0  \n","3  [[[253.0, 202.0, 245.0], [255.0, 205.0, 248.0]...         0  \n","4  [[[182.0, 144.0, 155.0], [183.0, 145.0, 156.0]...         0  \n","Number of rows in the DataFrame: 13879\n"]}],"source":["print(df.head())\n","row_count = len(df)\n","print(\"Number of rows in the DataFrame:\", row_count)"]},{"attachments":{},"cell_type":"markdown","id":"401082fc","metadata":{"papermill":{"duration":0.016977,"end_time":"2024-05-09T20:23:09.148992","exception":false,"start_time":"2024-05-09T20:23:09.132015","status":"completed"},"tags":[]},"source":["# Code to make augmentation of pics\n","needs to be put in order"]},{"cell_type":"code","execution_count":8,"id":"f3edf820","metadata":{"execution":{"iopub.execute_input":"2024-05-10T18:09:45.970020Z","iopub.status.busy":"2024-05-10T18:09:45.969685Z","iopub.status.idle":"2024-05-10T18:09:45.980497Z","shell.execute_reply":"2024-05-10T18:09:45.979563Z","shell.execute_reply.started":"2024-05-10T18:09:45.969993Z"},"papermill":{"duration":0.035787,"end_time":"2024-05-09T20:23:09.204724","exception":false,"start_time":"2024-05-09T20:23:09.168937","status":"completed"},"tags":[],"trusted":true},"outputs":[{"data":{"text/plain":["'original_dataset_dir = \\'./images/undetec_to_augm\\'\\ntrain_augm_undetected_dir = \\'./images/train/Undetected\\'\\ntest_augm_undetected_dir =\\'./images/test/Undetected\\'\\n\\n# Create a data generator for augmentation\\ndatagen = ImageDataGenerator(\\n    rotation_range=30,\\n    width_shift_range=0.2,\\n    height_shift_range=0.2,\\n    shear_range=0.2,\\n    zoom_range=0.2,\\n    horizontal_flip=True,\\n    brightness_range = [0.8, 1.3],\\n    fill_mode=\\'nearest\\'\\n)\\n\\n# List the files in the original dataset directory\\nfile_list = os.listdir(original_dataset_dir)\\n\\n# Ensure the target directories exist\\nos.makedirs(train_augm_undetected_dir, exist_ok=True)\\nos.makedirs(test_augm_undetected_dir, exist_ok=True)\\n\\n# Desired number of images after augmentation\\ntarget_count_1 = 6000\\ntarget_count_2 = 1000\\n\"\"\"= target_count_2 = target_count_3 = target_count_4 = target_count_5 = target_count_6 = target_count_7\"\"\"\\n\\n# Number of images in the original dataset\\noriginal_count = len(file_list)\\n\\n\\n# Number of times to repeat each image to reach the target count\\nrepeats_1 = min(target_count_1 // original_count + 1, len(file_list))\\nrepeats_2 = min(target_count_2 // original_count + 1, len(file_list))\\n\"\"\"repeats_3 = min(target_count_3 // original_count + 1, len(file_list))\\nrepeats_4 = min(target_count_4 // original_count + 1, len(file_list))\\nrepeats_5 = min(target_count_5 // original_count + 1, len(file_list))\\nrepeats_6 = min(target_count_6 // original_count + 1, len(file_list))\\nrepeats_7 = min(target_count_7 // original_count + 1, len(file_list))\"\"\"\\n\\n# Augment and save the images for the 6000 train images\\nfor file in file_list[:original_count]:\\n    img_path = os.path.join(original_dataset_dir, file)\\n    img = load_img(img_path)\\n    img = img_to_array(img)\\n    img = img.reshape((1,) + img.shape)\\n\\n    i = 0\\n    for batch in datagen.flow(img, batch_size=1, save_to_dir = train_augm_undetected_dir, save_prefix=\\'augm\\', save_format=\\'jpeg\\'): \\n        #if in the previous line i save them as jpg, it anyway augments only the jpeg ones\\n        i += 1\\n        if i >= repeats_1:\\n            break  # break the loop after reaching the desired number of augmented images\\n\\n\\n\\n# Augment and save the images for the 1000 test images\\nfor file in file_list[:original_count]:\\n    img_path = os.path.join(original_dataset_dir, file)\\n    img = load_img(img_path)\\n    img = img_to_array(img)\\n    img = img.reshape((1,) + img.shape)\\n\\n    i = 0\\n    for batch in datagen.flow(img, batch_size=1, save_to_dir = test_augm_undetected_dir, save_prefix=\\'aug\\', save_format=\\'jpeg\\'):\\n        i += 1\\n        if i >= repeats_2:\\n            break  # break the loop after reaching the desired number of augmented images '"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["'''original_dataset_dir = './images/undetec_to_augm'\n","train_augm_undetected_dir = './images/train/Undetected'\n","test_augm_undetected_dir ='./images/test/Undetected'\n","\n","# Create a data generator for augmentation\n","datagen = ImageDataGenerator(\n","    rotation_range=30,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    brightness_range = [0.8, 1.3],\n","    fill_mode='nearest'\n",")\n","\n","# List the files in the original dataset directory\n","file_list = os.listdir(original_dataset_dir)\n","\n","# Ensure the target directories exist\n","os.makedirs(train_augm_undetected_dir, exist_ok=True)\n","os.makedirs(test_augm_undetected_dir, exist_ok=True)\n","\n","# Desired number of images after augmentation\n","target_count_1 = 6000\n","target_count_2 = 1000\n","\"\"\"= target_count_2 = target_count_3 = target_count_4 = target_count_5 = target_count_6 = target_count_7\"\"\"\n","\n","# Number of images in the original dataset\n","original_count = len(file_list)\n","\n","\n","# Number of times to repeat each image to reach the target count\n","repeats_1 = min(target_count_1 // original_count + 1, len(file_list))\n","repeats_2 = min(target_count_2 // original_count + 1, len(file_list))\n","\"\"\"repeats_3 = min(target_count_3 // original_count + 1, len(file_list))\n","repeats_4 = min(target_count_4 // original_count + 1, len(file_list))\n","repeats_5 = min(target_count_5 // original_count + 1, len(file_list))\n","repeats_6 = min(target_count_6 // original_count + 1, len(file_list))\n","repeats_7 = min(target_count_7 // original_count + 1, len(file_list))\"\"\"\n","\n","# Augment and save the images for the 6000 train images\n","for file in file_list[:original_count]:\n","    img_path = os.path.join(original_dataset_dir, file)\n","    img = load_img(img_path)\n","    img = img_to_array(img)\n","    img = img.reshape((1,) + img.shape)\n","\n","    i = 0\n","    for batch in datagen.flow(img, batch_size=1, save_to_dir = train_augm_undetected_dir, save_prefix='augm', save_format='jpeg'): \n","        #if in the previous line i save them as jpg, it anyway augments only the jpeg ones\n","        i += 1\n","        if i >= repeats_1:\n","            break  # break the loop after reaching the desired number of augmented images\n","\n","\n","\n","# Augment and save the images for the 1000 test images\n","for file in file_list[:original_count]:\n","    img_path = os.path.join(original_dataset_dir, file)\n","    img = load_img(img_path)\n","    img = img_to_array(img)\n","    img = img.reshape((1,) + img.shape)\n","\n","    i = 0\n","    for batch in datagen.flow(img, batch_size=1, save_to_dir = test_augm_undetected_dir, save_prefix='aug', save_format='jpeg'):\n","        i += 1\n","        if i >= repeats_2:\n","            break  # break the loop after reaching the desired number of augmented images '''"]},{"cell_type":"code","execution_count":10,"id":"06ceba23","metadata":{"execution":{"iopub.execute_input":"2024-05-10T18:09:45.995871Z","iopub.status.busy":"2024-05-10T18:09:45.995568Z","iopub.status.idle":"2024-05-10T18:09:46.846707Z","shell.execute_reply":"2024-05-10T18:09:46.845757Z","shell.execute_reply.started":"2024-05-10T18:09:45.995848Z"},"trusted":true},"outputs":[],"source":["from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"code","execution_count":11,"id":"916d5035","metadata":{"execution":{"iopub.execute_input":"2024-05-10T18:09:46.848363Z","iopub.status.busy":"2024-05-10T18:09:46.847987Z","iopub.status.idle":"2024-05-10T18:09:49.275234Z","shell.execute_reply":"2024-05-10T18:09:49.274327Z","shell.execute_reply.started":"2024-05-10T18:09:46.848332Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[[[170. 129. 145.]\n","   [172. 131. 147.]\n","   [170. 129. 145.]\n","   ...\n","   [202. 167. 174.]\n","   [200. 165. 172.]\n","   [199. 164. 171.]]\n","\n","  [[169. 128. 144.]\n","   [171. 130. 146.]\n","   [172. 131. 147.]\n","   ...\n","   [202. 167. 174.]\n","   [200. 165. 172.]\n","   [199. 164. 171.]]\n","\n","  [[170. 129. 145.]\n","   [171. 130. 146.]\n","   [173. 132. 146.]\n","   ...\n","   [202. 167. 174.]\n","   [201. 166. 173.]\n","   [199. 164. 171.]]\n","\n","  ...\n","\n","  [[176. 144. 149.]\n","   [179. 147. 152.]\n","   [183. 148. 154.]\n","   ...\n","   [195. 159. 163.]\n","   [193. 157. 161.]\n","   [191. 155. 159.]]\n","\n","  [[174. 142. 147.]\n","   [177. 145. 150.]\n","   [182. 147. 153.]\n","   ...\n","   [196. 160. 164.]\n","   [193. 157. 161.]\n","   [191. 155. 159.]]\n","\n","  [[173. 141. 146.]\n","   [176. 144. 149.]\n","   [181. 146. 152.]\n","   ...\n","   [195. 159. 163.]\n","   [193. 157. 161.]\n","   [190. 154. 158.]]]\n","\n","\n"," [[[140. 108.  87.]\n","   [141. 109.  88.]\n","   [143. 111.  90.]\n","   ...\n","   [133. 100.  85.]\n","   [147. 114.  99.]\n","   [137. 104.  89.]]\n","\n","  [[140. 108.  87.]\n","   [141. 109.  88.]\n","   [143. 111.  90.]\n","   ...\n","   [134. 101.  86.]\n","   [147. 114.  99.]\n","   [138. 105.  90.]]\n","\n","  [[140. 108.  87.]\n","   [141. 109.  88.]\n","   [143. 111.  90.]\n","   ...\n","   [133. 100.  85.]\n","   [147. 114.  99.]\n","   [138. 105.  90.]]\n","\n","  ...\n","\n","  [[130. 113.  87.]\n","   [126. 109.  83.]\n","   [127. 110.  84.]\n","   ...\n","   [143. 117. 100.]\n","   [140. 114.  97.]\n","   [136. 110.  93.]]\n","\n","  [[130. 114.  88.]\n","   [125. 109.  83.]\n","   [126. 109.  83.]\n","   ...\n","   [143. 117. 100.]\n","   [140. 114.  97.]\n","   [134. 108.  91.]]\n","\n","  [[130. 114.  88.]\n","   [125. 109.  83.]\n","   [126. 109.  83.]\n","   ...\n","   [143. 117. 100.]\n","   [139. 113.  96.]\n","   [134. 108.  91.]]]\n","\n","\n"," [[[170. 130. 156.]\n","   [174. 134. 160.]\n","   [180. 140. 166.]\n","   ...\n","   [172. 125. 145.]\n","   [170. 125. 145.]\n","   [172. 127. 147.]]\n","\n","  [[173. 133. 159.]\n","   [176. 136. 162.]\n","   [179. 139. 165.]\n","   ...\n","   [175. 128. 148.]\n","   [177. 132. 152.]\n","   [181. 136. 156.]]\n","\n","  [[176. 136. 162.]\n","   [177. 137. 163.]\n","   [178. 138. 164.]\n","   ...\n","   [180. 133. 153.]\n","   [185. 140. 160.]\n","   [190. 145. 165.]]\n","\n","  ...\n","\n","  [[162. 120. 140.]\n","   [162. 120. 140.]\n","   [160. 118. 138.]\n","   ...\n","   [170. 121. 140.]\n","   [167. 120. 138.]\n","   [167. 120. 138.]]\n","\n","  [[162. 120. 140.]\n","   [162. 120. 140.]\n","   [159. 117. 137.]\n","   ...\n","   [168. 119. 138.]\n","   [167. 120. 138.]\n","   [171. 124. 142.]]\n","\n","  [[162. 120. 140.]\n","   [161. 119. 139.]\n","   [158. 116. 136.]\n","   ...\n","   [167. 118. 137.]\n","   [167. 120. 138.]\n","   [173. 126. 144.]]]\n","\n","\n"," ...\n","\n","\n"," [[[  0.   0.   0.]\n","   [  0.   0.   0.]\n","   [  0.   0.   0.]\n","   ...\n","   [  0.   0.   0.]\n","   [  0.   0.   0.]\n","   [  0.   0.   0.]]\n","\n","  [[  0.   0.   0.]\n","   [  0.   0.   0.]\n","   [  0.   0.   0.]\n","   ...\n","   [  0.   0.   0.]\n","   [  0.   0.   0.]\n","   [  0.   0.   0.]]\n","\n","  [[  0.   0.   0.]\n","   [  0.   0.   0.]\n","   [  0.   0.   0.]\n","   ...\n","   [  0.   0.   0.]\n","   [  0.   0.   0.]\n","   [  0.   0.   0.]]\n","\n","  ...\n","\n","  [[  0.   0.   0.]\n","   [  0.   0.   0.]\n","   [  0.   0.   0.]\n","   ...\n","   [  1.   1.   1.]\n","   [  1.   1.   1.]\n","   [  1.   1.   1.]]\n","\n","  [[  0.   0.   0.]\n","   [  0.   0.   0.]\n","   [  0.   0.   0.]\n","   ...\n","   [  1.   1.   1.]\n","   [  1.   1.   1.]\n","   [  1.   1.   1.]]\n","\n","  [[  0.   0.   0.]\n","   [  0.   0.   0.]\n","   [  0.   0.   0.]\n","   ...\n","   [  1.   1.   1.]\n","   [  1.   1.   1.]\n","   [  1.   1.   1.]]]\n","\n","\n"," [[[158. 136. 138.]\n","   [152. 131. 128.]\n","   [151. 126. 121.]\n","   ...\n","   [145. 133. 121.]\n","   [135. 122. 114.]\n","   [131. 121. 112.]]\n","\n","  [[158. 136. 138.]\n","   [153. 129. 127.]\n","   [150. 125. 118.]\n","   ...\n","   [154. 140. 129.]\n","   [134. 121. 113.]\n","   [120. 110. 101.]]\n","\n","  [[159. 135. 135.]\n","   [152. 128. 126.]\n","   [148. 123. 116.]\n","   ...\n","   [161. 147. 136.]\n","   [140. 127. 119.]\n","   [121. 108. 100.]]\n","\n","  ...\n","\n","  [[163. 148. 145.]\n","   [164. 146. 144.]\n","   [164. 140. 138.]\n","   ...\n","   [145. 127. 115.]\n","   [140. 122. 110.]\n","   [139. 121. 109.]]\n","\n","  [[162. 146. 146.]\n","   [164. 146. 144.]\n","   [165. 141. 139.]\n","   ...\n","   [140. 122. 110.]\n","   [138. 120. 108.]\n","   [141. 123. 111.]]\n","\n","  [[161. 145. 145.]\n","   [163. 145. 143.]\n","   [166. 142. 142.]\n","   ...\n","   [137. 119. 107.]\n","   [136. 118. 106.]\n","   [143. 125. 113.]]]\n","\n","\n"," [[[ 46.  51.  57.]\n","   [ 51.  54.  59.]\n","   [ 77.  72.  76.]\n","   ...\n","   [117. 114. 109.]\n","   [121. 121. 119.]\n","   [121. 126. 122.]]\n","\n","  [[ 48.  53.  59.]\n","   [ 59.  60.  65.]\n","   [ 78.  73.  77.]\n","   ...\n","   [129. 124. 120.]\n","   [133. 134. 129.]\n","   [124. 129. 125.]]\n","\n","  [[ 53.  56.  61.]\n","   [ 61.  62.  66.]\n","   [ 70.  66.  67.]\n","   ...\n","   [130. 125. 121.]\n","   [134. 133. 129.]\n","   [121. 123. 118.]]\n","\n","  ...\n","\n","  [[193. 194. 189.]\n","   [192. 193. 188.]\n","   [188. 187. 182.]\n","   ...\n","   [ 57.  53.  54.]\n","   [ 77.  66.  72.]\n","   [120. 107. 114.]]\n","\n","  [[191. 193. 188.]\n","   [192. 193. 188.]\n","   [188. 187. 183.]\n","   ...\n","   [ 63.  59.  60.]\n","   [ 96.  85.  91.]\n","   [105.  92. 101.]]\n","\n","  [[191. 193. 188.]\n","   [189. 191. 186.]\n","   [185. 184. 180.]\n","   ...\n","   [ 78.  76.  77.]\n","   [121. 110. 116.]\n","   [100.  87.  96.]]]]\n"]}],"source":["# Making sure it's a 2D array\n","image_matrix = np.stack(df['image_array'].values)\n","print(image_matrix)"]},{"cell_type":"code","execution_count":null,"id":"0b07fdf8","metadata":{},"outputs":[],"source":["# It's a three dimensions array so we have to flatten it \n","image_matrix = np.array([img.flatten() for img in df['image_array']])\n","\n","# Scale the data\n","scaler = StandardScaler()\n","image_matrix_scaled = scaler.fit_transform(image_matrix)"]},{"cell_type":"code","execution_count":null,"id":"319d7906","metadata":{"execution":{"iopub.execute_input":"2024-05-10T18:09:49.276916Z","iopub.status.busy":"2024-05-10T18:09:49.276554Z"},"papermill":{"duration":0.020456,"end_time":"2024-05-09T20:23:09.245954","exception":false,"start_time":"2024-05-09T20:23:09.225498","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Principal Component Analysis\n","pca_ratio=PCA()\n","pca_ratio.fit(image_matrix_scaled) # fit the PCA so it can learn\n","\n","# Using cumulative variance\n","cumulative_variance_ratio=np.cumsum(pca_ratio.explained_variance_ratio_)\n","variance=0.95 # set to 95% to keep a sufficiently large portion of the variance\n","n_components= np.argmax(cumulative_variance_ratio >= variance) +1 # find the number of components needed \n","\n","print(f\"Number of principal components: {n_components}\")\n","\n","# There is another way to obtain the number of components which is setting the threshold \n","# pca_threshold = PCA(n_components=0.95)\n","# pca_threshold.fit(image_matrix_scaled) - fit the pca model to the data to learn patterns\n","# print(f\"Number of principal components: {pca_threshold.n_components_}\")"]},{"attachments":{},"cell_type":"markdown","id":"71316f40","metadata":{},"source":["Number of principal components: 72"]},{"cell_type":"code","execution_count":null,"id":"43f7ec86","metadata":{},"outputs":[],"source":["\n","# Screen plot eigenvalues - number of principal components\n","plt.figure(figsize=(10, 6))\n","sns.lineplot(x=np.arange(1, len(cumulative_variance_ratio) + 1), y=cumulative_variance_ratio, marker='o', color='#FF69B4')\n","plt.title('Scree Plot')\n","plt.xlabel('Number of Components')\n","plt.ylabel('Cumulative Explained Variance Ratio')\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"e7db175a","metadata":{"papermill":{"duration":0.020219,"end_time":"2024-05-09T20:23:09.286611","exception":false,"start_time":"2024-05-09T20:23:09.266392","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Transform the original data using retained principal components \n","# Start from inputting in PCA the number of components found necessary for 95% variance\n","pca_opt=PCA(n_components=72)\n","pca_opt.fit(image_matrix_scaled)\n","\n","df_matrix_reduced= pca_opt.transform(image_matrix_scaled) # transform the flatten original data to reduced dimensionality"]},{"attachments":{},"cell_type":"markdown","id":"adb332d2","metadata":{},"source":["# SVM with PCA"]},{"cell_type":"code","execution_count":null,"id":"405a9932","metadata":{},"outputs":[],"source":["from sklearn.svm import SVC\n","from sklearn.metrics import classification_report, accuracy_score\n","from sklearn.model_selection import GridSearchCV\n","\n","# Split the Data in test and train\n","train_indices = df[df['subset'] == 'train'].index\n","test_indices = df[df['subset'] == 'test'].index\n","\n","X_train = df_matrix_reduced[train_indices]\n","X_test = df_matrix_reduced[test_indices]\n","y_train = df.loc[train_indices, 'category']\n","y_test = df.loc[test_indices, 'category']\n","\n","# Definition of the parameter grid for GridSearchCV\n","param_grid = {\n","    'C': [0.1, 1, 10, 100],\n","    'gamma': [0.0001, 0.001, 0.1, 1],\n","    'kernel': ['rbf', 'poly']\n","}\n","\n","# Creation of an SVC\n","svc = SVC(probability=True)\n","\n","# Creating the model using GridSearchCV with the parameter grid\n","model = GridSearchCV(svc, param_grid, cv=5)\n","\n","# Training the model using training data\n","model.fit(X_train, y_train)\n","\n","# Testing the model using test data\n","y_pred = model.predict(X_test)\n","\n","# Printing the classification report\n","print(classification_report(y_test, y_pred))"]},{"cell_type":"markdown","id":"309af1a2","metadata":{},"source":["## SVM without PCA"]},{"cell_type":"code","execution_count":null,"id":"87d3424a","metadata":{},"outputs":[],"source":["# SVM without PCA\n","X_train = image_matrix_scaled[df['subset'] == 'train']\n","y_train = df[df['subset'] == 'train']['category']\n","X_test = image_matrix_scaled[df['subset'] == 'test']\n","y_test = df[df['subset'] == 'test']['category']\n","\n","# Definition of the parameter grid for GridSearchCV\n","param_grid = {\n","    'C': [0.1, 1, 10, 100],\n","    'gamma': [0.0001, 0.001, 0.1, 1],\n","    'kernel': ['rbf', 'poly']\n","}\n","\n","# Creation of an SVC\n","svc = SVC(probability=True)\n","\n","# Creating the model using GridSearchCV with the parameter grid\n","model = GridSearchCV(svc, param_grid, cv=5)\n","\n","# Training the model using training data\n","model.fit(X_train, y_train)\n","\n","# Testing the model using test data\n","y_pred = model.predict(X_test)\n","\n","# Printing the classification report\n","print(classification_report(y_test, y_pred))"]},{"cell_type":"markdown","id":"a5149001","metadata":{},"source":["# SVM without PCA small sample"]},{"cell_type":"code","execution_count":null,"id":"e9d5402a","metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Indici dei dati di train e test come definiti nella colonna 'subset'\n","train_indices = df[df['subset'] == 'train'].index\n","test_indices = df[df['subset'] == 'test'].index\n","\n","# Selezionare il 20% casuale da entrambi i subset di training e test\n","train_indices_sample = train_test_split(train_indices, train_size=0.2, random_state=42)[0]\n","test_indices_sample = train_test_split(test_indices, test_size=0.2, random_state=42)[1]\n","\n","# Assegnare le features e i target basandosi sugli indici selezionati\n","X_train = image_matrix_scaled[train_indices_sample]\n","X_test = image_matrix_scaled[test_indices_sample]\n","y_train = df.loc[train_indices_sample, 'category']\n","y_test = df.loc[test_indices_sample, 'category']\n","\n","# Creazione e configurazione del modello SVM con GridSearchCV\n","from sklearn.svm import SVC\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import GridSearchCV\n","\n","param_grid = {\n","    'C': [0.1, 1, 10, 100],\n","    'gamma': [0.0001, 0.001, 0.1, 1],\n","    'kernel': ['rbf', 'poly']\n","}\n","\n","svc = SVC(probability=True)\n","model = GridSearchCV(svc, param_grid, cv=5)\n","model.fit(X_train, y_train)\n","\n","# Suddivisione dei dati di test in 4 batch e classificazione\n","batch_size = len(X_test) // 4  # Determina la dimensione del batch in base al numero totale di campioni nel test set\n","\n","for i in range(4):\n","    start_index = i * batch_size\n","    end_index = start_index + batch_size if i < 3 else len(X_test)  # Assicurati che l'ultimo batch includa tutti i rimanenti dati\n","\n","    X_batch = X_test[start_index:end_index]\n","    y_batch = y_test[start_index:end_index]\n","    \n","    # Predizione del modello sul batch\n","    y_pred_batch = model.predict(X_batch)\n","    \n","    # Stampa del report di classificazione per il batch corrente\n","    print(f\"Classification Report for Batch {i+1}\")\n","    print(classification_report(y_batch, y_pred_batch))\n","    print(\"\\n\" + \"-\"*80 + \"\\n\")\n"]},{"cell_type":"markdown","id":"a6397a29","metadata":{},"source":["# SVM without PCA with SGDC Classifier"]},{"cell_type":"code","execution_count":null,"id":"08502ffd","metadata":{},"outputs":[],"source":["from sklearn.linear_model import SGDClassifier\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import GridSearchCV\n","\n","# SVM without PCA\n","X_train = image_matrix_scaled[df['subset'] == 'train']\n","y_train = df[df['subset'] == 'train']['category']\n","X_test = image_matrix_scaled[df['subset'] == 'test']\n","y_test = df[df['subset'] == 'test']['category']\n","\n","# Define parameter grid for SGDClassifier\n","param_grid_sgd = {\n","    'alpha': [0.0001, 0.001, 0.01, 0.1],  # Regularization parameter\n","    'penalty': ['l2', 'l1', 'elasticnet'],\n","    'loss': ['hinge']  # Hinge loss corresponds to a linear SVM\n","}\n","\n","# Create SGDClassifier\n","sgd = SGDClassifier(max_iter=1000, tol=1e-3)\n","\n","# Create GridSearchCV model\n","grid_search_sgd = GridSearchCV(sgd, param_grid_sgd, cv=5)\n","\n","# Train model using training data\n","grid_search_sgd.fit(X_train, y_train)\n","\n","# Test model using test data\n","y_pred_sgd = grid_search_sgd.predict(X_test)\n","\n","# Print classification report\n","print(classification_report(y_test, y_pred_sgd))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"},"papermill":{"default_parameters":{},"duration":69.191232,"end_time":"2024-05-09T20:23:11.684837","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-09T20:22:02.493605","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}
